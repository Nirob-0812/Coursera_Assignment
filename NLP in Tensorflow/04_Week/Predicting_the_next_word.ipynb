{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,Bidirectional,LSTM,Dense\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nirob\\anaconda3\\envs\\Myenv310\\lib\\site-packages\\gdown\\__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=108jAePKK4R3BVYBbYJZ32JWUwxeMg20K\n",
      "To: f:\\Code\\NLP\\04_Week\\sonnets.txt\n",
      "\n",
      "  0%|          | 0.00/93.6k [00:00<?, ?B/s]\n",
      "100%|██████████| 93.6k/93.6k [00:00<00:00, 612kB/s]\n",
      "100%|██████████| 93.6k/93.6k [00:00<00:00, 612kB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 108jAePKK4R3BVYBbYJZ32JWUwxeMg20K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='F:/Code/NLP/04_Week/sonnets.txt'\n",
    "with open(data_path) as f:\n",
    "    data=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=data.lower().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2159\n",
      "from fairest creatures we desire increase,\n",
      "that thereby beauty's rose might never die,\n",
      "but as the riper should by time decease,\n",
      "his tender heir might bear his memory:\n",
      "but thou, contracted to thine own bright eyes,\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "print( '\\n'.join(corpus[i] for i in range(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3211"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words=len(tokenizer.word_index)+1\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from fairest creatures we desire increase,'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [58],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [17],\n",
       " [6],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [17],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [6],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [6],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [17],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[34, 417, 877, 166, 213, 517]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([corpus[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34, 417, 877, 166, 213, 517]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([corpus[0]])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_seqs(corpus, tokenizer):\n",
    "    \"\"\"\n",
    "    Generates a list of n-gram sequences\n",
    "    \n",
    "    Args:\n",
    "        corpus (list of string): lines of texts to generate n-grams for\n",
    "        tokenizer (object): an instance of the Tokenizer class containing the word-index dictionary\n",
    "    \n",
    "    Returns:\n",
    "        input_sequences (list of int): the n-gram sequences for each line in the corpus\n",
    "    \"\"\"\n",
    "    input_sequences = []\n",
    "\n",
    "    for i in corpus:\n",
    "        token=tokenizer.texts_to_sequences([i])[0]\n",
    "        for i in range(1,len(token)):\n",
    "            n_gram_token=token[:i+1]\n",
    "            input_sequences.append(n_gram_token)\n",
    "\n",
    "    \n",
    "    return input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_gram sequences for first example look like this:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[34, 417],\n",
       " [34, 417, 877],\n",
       " [34, 417, 877, 166],\n",
       " [34, 417, 877, 166, 213],\n",
       " [34, 417, 877, 166, 213, 517]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_example_sequence = n_gram_seqs([corpus[0]], tokenizer)\n",
    "\n",
    "print(\"n_gram sequences for first example look like this:\\n\")\n",
    "first_example_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[34, 417],\n",
       " [34, 417, 877],\n",
       " [34, 417, 877, 166],\n",
       " [34, 417, 877, 166, 213],\n",
       " [34, 417, 877, 166, 213, 517]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_1=n_gram_seqs(corpus[:1], tokenizer)\n",
    "n_gram_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 878],\n",
       " [8, 878, 134],\n",
       " [8, 878, 134, 351],\n",
       " [8, 878, 134, 351, 102],\n",
       " [8, 878, 134, 351, 102, 156],\n",
       " [8, 878, 134, 351, 102, 156, 199],\n",
       " [16, 22],\n",
       " [16, 22, 2],\n",
       " [16, 22, 2, 879],\n",
       " [16, 22, 2, 879, 61],\n",
       " [16, 22, 2, 879, 61, 30],\n",
       " [16, 22, 2, 879, 61, 30, 48],\n",
       " [16, 22, 2, 879, 61, 30, 48, 634],\n",
       " [25, 311],\n",
       " [25, 311, 635],\n",
       " [25, 311, 635, 102],\n",
       " [25, 311, 635, 102, 200],\n",
       " [25, 311, 635, 102, 200, 25],\n",
       " [25, 311, 635, 102, 200, 25, 278]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_3_examples=n_gram_seqs(corpus[1:4],tokenizer)\n",
    "next_3_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next_3_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15462\n"
     ]
    }
   ],
   "source": [
    "# n_grams of input_sequences have length:\n",
    "input_sequences=n_gram_seqs(corpus,tokenizer)\n",
    "print(len(input_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maximum length of sequences is:\n",
    "max_len=max(len(i)  for i in input_sequences)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padded Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,  34, 417],\n",
       "       [  0,   0,   0,  34, 417, 877],\n",
       "       [  0,   0,  34, 417, 877, 166],\n",
       "       [  0,  34, 417, 877, 166, 213],\n",
       "       [ 34, 417, 877, 166, 213, 517]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_seq=pad_sequences(first_example_sequence,maxlen=max(len(i) for i in first_example_sequence))\n",
    "padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   8, 878],\n",
       "       [  0,   0,   0,   0,   0,   8, 878, 134],\n",
       "       [  0,   0,   0,   0,   8, 878, 134, 351],\n",
       "       [  0,   0,   0,   8, 878, 134, 351, 102],\n",
       "       [  0,   0,   8, 878, 134, 351, 102, 156],\n",
       "       [  0,   8, 878, 134, 351, 102, 156, 199],\n",
       "       [  0,   0,   0,   0,   0,   0,  16,  22],\n",
       "       [  0,   0,   0,   0,   0,  16,  22,   2],\n",
       "       [  0,   0,   0,   0,  16,  22,   2, 879],\n",
       "       [  0,   0,   0,  16,  22,   2, 879,  61],\n",
       "       [  0,   0,  16,  22,   2, 879,  61,  30],\n",
       "       [  0,  16,  22,   2, 879,  61,  30,  48],\n",
       "       [ 16,  22,   2, 879,  61,  30,  48, 634],\n",
       "       [  0,   0,   0,   0,   0,   0,  25, 311],\n",
       "       [  0,   0,   0,   0,   0,  25, 311, 635],\n",
       "       [  0,   0,   0,   0,  25, 311, 635, 102],\n",
       "       [  0,   0,   0,  25, 311, 635, 102, 200],\n",
       "       [  0,   0,  25, 311, 635, 102, 200,  25],\n",
       "       [  0,  25, 311, 635, 102, 200,  25, 278]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_seq_next_3=pad_sequences(next_3_examples,maxlen=max(len(i) for i in next_3_examples))\n",
    "padded_seq_next_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15462, 11)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Padded whole Corpus\n",
    "max_sequence_length=max(len(i)  for i in input_sequences)\n",
    "padded_whole_corpus=pad_sequences(input_sequences,maxlen=max_sequence_length)\n",
    "padded_whole_corpus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting into features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3211)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,  34],\n",
       "       [  0,   0,   0,  34, 417],\n",
       "       [  0,   0,  34, 417, 877],\n",
       "       [  0,  34, 417, 877, 166],\n",
       "       [ 34, 417, 877, 166, 213]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature and labels for example 1\n",
    "features_for_1=padded_seq[:,:-1]\n",
    "labels_for_1=padded_seq[:,-1]\n",
    "one_hot_encode_for_1=to_categorical(labels_for_1,total_words)\n",
    "print(one_hot_encode_for_1.shape)\n",
    "features_for_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15462, 10)\n",
      "(15462, 3211)\n"
     ]
    }
   ],
   "source": [
    "features=padded_whole_corpus[:,:-1]\n",
    "labels=padded_whole_corpus[:,-1]\n",
    "one_hot_encode=to_categorical(labels,total_words)\n",
    "print(features.shape)\n",
    "print(one_hot_encode.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(total_words,100,input_length=max_sequence_length-1))\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(total_words,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "484/484 [==============================] - 19s 29ms/step - loss: 6.8953 - accuracy: 0.0213\n",
      "Epoch 2/50\n",
      "484/484 [==============================] - 14s 29ms/step - loss: 6.4363 - accuracy: 0.0312\n",
      "Epoch 3/50\n",
      "484/484 [==============================] - 13s 27ms/step - loss: 6.2021 - accuracy: 0.0410\n",
      "Epoch 4/50\n",
      "484/484 [==============================] - 12s 25ms/step - loss: 5.9453 - accuracy: 0.0511\n",
      "Epoch 5/50\n",
      "484/484 [==============================] - 12s 25ms/step - loss: 5.6464 - accuracy: 0.0644\n",
      "Epoch 6/50\n",
      "484/484 [==============================] - 12s 25ms/step - loss: 5.3031 - accuracy: 0.0747\n",
      "Epoch 7/50\n",
      "484/484 [==============================] - 13s 26ms/step - loss: 4.9273 - accuracy: 0.0906\n",
      "Epoch 8/50\n",
      "484/484 [==============================] - 13s 27ms/step - loss: 4.5257 - accuracy: 0.1193\n",
      "Epoch 9/50\n",
      "484/484 [==============================] - 13s 26ms/step - loss: 4.1128 - accuracy: 0.1717\n",
      "Epoch 10/50\n",
      "484/484 [==============================] - 12s 25ms/step - loss: 3.7083 - accuracy: 0.2393\n",
      "Epoch 11/50\n",
      "484/484 [==============================] - 13s 26ms/step - loss: 3.3289 - accuracy: 0.3022\n",
      "Epoch 12/50\n",
      "484/484 [==============================] - 13s 26ms/step - loss: 2.9826 - accuracy: 0.3760\n",
      "Epoch 13/50\n",
      "484/484 [==============================] - 13s 26ms/step - loss: 2.6769 - accuracy: 0.4308\n",
      "Epoch 14/50\n",
      "484/484 [==============================] - 13s 27ms/step - loss: 2.4081 - accuracy: 0.4898\n",
      "Epoch 15/50\n",
      "484/484 [==============================] - 13s 27ms/step - loss: 2.1654 - accuracy: 0.5392\n",
      "Epoch 16/50\n",
      "484/484 [==============================] - 13s 27ms/step - loss: 1.9544 - accuracy: 0.5878\n",
      "Epoch 17/50\n",
      "484/484 [==============================] - 13s 26ms/step - loss: 1.7745 - accuracy: 0.6303\n",
      "Epoch 18/50\n",
      "484/484 [==============================] - 13s 27ms/step - loss: 1.6117 - accuracy: 0.6656\n",
      "Epoch 19/50\n",
      "484/484 [==============================] - 13s 27ms/step - loss: 1.4630 - accuracy: 0.6972\n",
      "Epoch 20/50\n",
      "484/484 [==============================] - 13s 26ms/step - loss: 1.3336 - accuracy: 0.7259\n",
      "Epoch 21/50\n",
      "484/484 [==============================] - 13s 27ms/step - loss: 1.2179 - accuracy: 0.7536\n",
      "Epoch 22/50\n",
      "484/484 [==============================] - 13s 26ms/step - loss: 1.1229 - accuracy: 0.7723\n",
      "Epoch 23/50\n",
      "484/484 [==============================] - 12s 26ms/step - loss: 1.0399 - accuracy: 0.7856\n",
      "Epoch 24/50\n",
      "484/484 [==============================] - 12s 25ms/step - loss: 0.9639 - accuracy: 0.8017\n",
      "Epoch 25/50\n",
      "484/484 [==============================] - 12s 25ms/step - loss: 0.9026 - accuracy: 0.8119\n",
      "Epoch 26/50\n",
      "484/484 [==============================] - 12s 25ms/step - loss: 0.8509 - accuracy: 0.8192\n",
      "Epoch 27/50\n",
      "484/484 [==============================] - 12s 26ms/step - loss: 0.8097 - accuracy: 0.8273\n",
      "Epoch 28/50\n",
      "484/484 [==============================] - 12s 26ms/step - loss: 0.7693 - accuracy: 0.8309\n",
      "Epoch 29/50\n",
      "484/484 [==============================] - 18s 37ms/step - loss: 0.7401 - accuracy: 0.8345\n",
      "Epoch 30/50\n",
      "484/484 [==============================] - 15s 31ms/step - loss: 0.7133 - accuracy: 0.8388\n",
      "Epoch 31/50\n",
      "484/484 [==============================] - 13s 27ms/step - loss: 0.6956 - accuracy: 0.8384\n",
      "Epoch 32/50\n",
      "484/484 [==============================] - 13s 26ms/step - loss: 0.6700 - accuracy: 0.8426\n",
      "Epoch 33/50\n",
      "484/484 [==============================] - 13s 26ms/step - loss: 0.6545 - accuracy: 0.8461\n",
      "Epoch 34/50\n",
      "484/484 [==============================] - 12s 26ms/step - loss: 0.6391 - accuracy: 0.8472\n",
      "Epoch 35/50\n",
      "484/484 [==============================] - 12s 25ms/step - loss: 0.6304 - accuracy: 0.8479\n",
      "Epoch 36/50\n",
      "484/484 [==============================] - 12s 26ms/step - loss: 0.6256 - accuracy: 0.8472\n",
      "Epoch 37/50\n",
      "484/484 [==============================] - 12s 26ms/step - loss: 0.6225 - accuracy: 0.8474\n",
      "Epoch 38/50\n",
      "484/484 [==============================] - 12s 26ms/step - loss: 0.6104 - accuracy: 0.8481\n",
      "Epoch 39/50\n",
      "484/484 [==============================] - 13s 26ms/step - loss: 0.5984 - accuracy: 0.8489\n",
      "Epoch 40/50\n",
      "484/484 [==============================] - 13s 28ms/step - loss: 0.5946 - accuracy: 0.8483\n",
      "Epoch 41/50\n",
      "484/484 [==============================] - 15s 31ms/step - loss: 0.5986 - accuracy: 0.8463\n",
      "Epoch 42/50\n",
      "484/484 [==============================] - 13s 27ms/step - loss: 0.5877 - accuracy: 0.8492\n",
      "Epoch 43/50\n",
      "484/484 [==============================] - 14s 29ms/step - loss: 0.5849 - accuracy: 0.8493\n",
      "Epoch 44/50\n",
      "484/484 [==============================] - 13s 27ms/step - loss: 0.5779 - accuracy: 0.8496\n",
      "Epoch 45/50\n",
      "484/484 [==============================] - 13s 27ms/step - loss: 0.5751 - accuracy: 0.8492\n",
      "Epoch 46/50\n",
      "484/484 [==============================] - 13s 27ms/step - loss: 0.5691 - accuracy: 0.8499\n",
      "Epoch 47/50\n",
      "484/484 [==============================] - 13s 27ms/step - loss: 0.5683 - accuracy: 0.8509\n",
      "Epoch 48/50\n",
      "484/484 [==============================] - 13s 27ms/step - loss: 0.5708 - accuracy: 0.8492\n",
      "Epoch 49/50\n",
      "484/484 [==============================] - 13s 26ms/step - loss: 0.5623 - accuracy: 0.8509\n",
      "Epoch 50/50\n",
      "484/484 [==============================] - 12s 26ms/step - loss: 0.5630 - accuracy: 0.8492\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(features,one_hot_encode,epochs=50,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "acc=history.history['accuracy']\n",
    "loss=history.history['loss']\n",
    "epochs=range(len(acc))\n",
    "plt.plot(epochs,acc,'b',label='Training Accuracy')\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Myenv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
